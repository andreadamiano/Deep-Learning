{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNE2EddBqIz03K/YYXggvKZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Natural Lenguage Processing\n"],"metadata":{"id":"IueAPyGavoD1"}},{"cell_type":"code","source":["#libraries\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import tiktoken\n","import torch\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"7iQoIizXRJeo","executionInfo":{"status":"ok","timestamp":1749419716385,"user_tz":-120,"elapsed":7203,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["##Data Preprocessing"],"metadata":{"id":"Ql7NZ3knIJzC"}},{"cell_type":"code","source":["from warnings import simplefilter\n","#get some text\n","html = requests.get('https://en.wikisource.org/wiki/The_Verdict')\n","bs = BeautifulSoup(html.text, 'html.parser')\n","text = bs.find_all('p')\n","text = ' '.join(p.text for p in text)\n","# print(text)\n","\n","\n","#generate word tokens\n","tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","tokens = [token.strip() for token in tokens  if token.strip()]\n","# print(tokens)\n","\n","\n","#convert token into token IDs (integers)\n","\n","#convert the tokens into a set of sorted unique tokens\n","all_words = sorted(set(tokens))\n","all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])  #add 2 tokens , one for signaling the end of a text and another for unkown words\n","\n","#create a dictionary mapping tokens to token IDs\n","vocab_size = len(all_words)\n","vocab = {index : word for word, index in enumerate(all_words)}\n","\n","\n","class SimpleTokenizer:\n","    \"\"\"\n","    tokenizer that convert a string into token IDs and viceversa\n","    \"\"\"\n","\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {index : string for string , index in vocab.items()}\n","\n","\n","    def encode(self, text):\n","        #generate word tokens\n","        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","\n","        #clean tokens (get rid of extra ' ')\n","        tokens = [token.strip() for token in tokens if token.strip() ]\n","\n","        #if the item is not present in the vocab use the unknow token\n","        tokens = [token  if token in self.str_to_int else '<|unk|>' for token in tokens ]\n","\n","        #convert text into token ids\n","        ids = [self.str_to_int[token] for token in tokens ]\n","\n","        return ids\n","\n","\n","    def decode(self, ids):\n","        #get words from ids\n","        text = ' '.join([self.int_to_str[id] for id in  ids])\n","\n","        #eliminate spaces between punctuation\n","        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n","\n","        return text\n","\n","\n","\n","#examples\n","# text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n","\n","\n","# text1 = \"Hello, do you like tea?\"\n","# text2 = \"In the sunlit terraces of the palace.\"\n","# text = \" <|endoftext|> \".join((text1, text2))\n","\n","# tokenizer = SimpleTokenizer(vocab)\n","# print(tokenizer.encode(text))\n","# print(tokenizer.decode(tokenizer.encode(text)))\n","\n","\n","#byte pair tokenization\n","# tokenizer = tiktoken.get_encoding('gpt2')\n","# text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\")\n","# print(tokenizer.encode(text, allowed_special={'<|endoftext|>'}))\n","# print(tokenizer.decode(tokenizer.encode(text, allowed_special={'<|endoftext|>'})))\n","\n","\n","#create dataset\n","class GPTDataset(Dataset):\n","    def __init__(self, text, tokenizer, max_lenght, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        #tokenize the raw text\n","        token_ids = tokenizer.encode(text)\n","\n","        #create sliding windows\n","        for i in range (0, len(tokens) - max_lenght, stride):\n","            input_chunk = token_ids[i:i+max_lenght]\n","            target_chunk = token_ids[i+1 : i+max_lenght +1] #during pre training the gpt predicts all tokens shifted by one at the same time\n","\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len (self.input_ids)\n","\n","    def __getitem__(self, index):\n","        return self.input_ids[index] , self.target_ids[index]\n","\n","\n","#create dataloaders\n","def create_dataloader(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n","    tokenizer = tiktoken.get_encoding('gpt2')\n","    dataset = GPTDataset(text, tokenizer, max_length, stride)\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers,\n","    )\n","\n","    return dataloader\n","\n","\n","\n","dataloader = create_dataloader(text, batch_size=1, max_length=4, stride=1, shuffle= False, )\n","\n","# for batch_index , batch_tuple in enumerate(dataloader):\n","#     print(batch_tuple)\n","#     break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGE8KM1vRixb","executionInfo":{"status":"ok","timestamp":1749420442758,"user_tz":-120,"elapsed":176,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}},"outputId":"6b7f52f2-46e7-433e-e52f-a08b0aa2d7f5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"]}]}]}